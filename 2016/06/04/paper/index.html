<!doctype html>
<html class="theme-next use-motion theme-next-mist">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>






  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>


<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.3"/>




  <meta name="keywords" content="Hexo,next" />





  <link rel="shorticon icon" type="image/x-icon" href="/favicon.ico?v=0.4.3" />



<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: 'Mist',
    analytics: {
      google: ''
    },
    sidebar: 'post'
  };
</script>




  <title>  // Sunshine </title>
</head>

<body>
<!--[if lte IE 8]> <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'> <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode"><img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820" alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari." style='margin-left:auto;margin-right:auto;display: block;'/></a></div> <![endif]-->
  <div class="container one-column page-post-detail">
    <div class="headband"></div>

    <div id="header" class="header">
      <div class="header-inner">
        <h1 class="site-meta">
  <span class="logo-line-before"><i></i></span>
  <a href="/" class="brand">
      <span class="logo">
        <i class="icon-logo"></i>
      </span>
      <span class="site-title">Sunshine</span>
  </a>
  <span class="logo-line-after"><i></i></span>
</h1>


  <ul id="menu" class="menu">
     
    
      
      <li class="menu-item menu-item-home">
        <a href="/">
          <i class="menu-item-icon icon-home"></i> <br />
          首页
        </a>
      </li>
    
      
      <li class="menu-item menu-item-archives">
        <a href="/archives">
          <i class="menu-item-icon icon-archives"></i> <br />
          归档
        </a>
      </li>
    
      
      <li class="menu-item menu-item-categories">
        <a href="/categories">
          <i class="menu-item-icon icon-categories"></i> <br />
          分类
        </a>
      </li>
    
      
      <li class="menu-item menu-item-tags">
        <a href="/tags">
          <i class="menu-item-icon icon-tags"></i> <br />
          标签
        </a>
      </li>
    
      
      <li class="menu-item menu-item-about">
        <a href="/about">
          <i class="menu-item-icon icon-about"></i> <br />
          关于
        </a>
      </li>
    
  </ul>


      </div>
    </div>

    <div id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content">
          

  <div id="posts" class="posts-expand">
    

  <div class="post post-type-normal ">
    <div class="post-header">

      
      
        <h1 class="post-title">
          
          
            
              
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于 2016-06-04
        </span>

        

        
          
            <span class="post-comments-count">
            &nbsp; | &nbsp;
            <a href="/2016/06/04/paper/#comments" >
              <span class="post-comments-count ds-thread-count" data-thread-key="2016/06/04/paper/"></span>
            </a>
          </span>
          
        
      </div>
    </div>

    <div class="post-body">

      
      

      
        <h1 id="Awesome_-_Most_Cited_Deep_Learning_Papers">Awesome - Most Cited Deep Learning Papers</h1><p><a href="https://github.com/sindresorhus/awesome" target="_blank" rel="external"><img src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" alt="Awesome"></a></p>
<p>A curated list of the most cited deep learning papers (since 2010)</p>
<p>I believe that there exist <em>classic</em> deep learning papers which are worth reading regardless of their applications. Rather than providing overwhelming amount of papers, I would like to provide a <em>curated list</em> of the classic deep learning papers which can be considered as <em>must-reads</em> in some area. </p>
<h2 id="Awesome_list_criteria">Awesome list criteria</h2><ul>
<li><strong>2016</strong> :  Based on discussions</li>
<li><strong>2015</strong> :  +100 citations (:sparkles: +200)</li>
<li><strong>2014</strong> :  +200 citations (:sparkles: +400)</li>
<li><strong>2013</strong> :  +300 citations (:sparkles: +600)</li>
<li><strong>2012</strong> :  +400 citations (:sparkles: +800)</li>
<li><strong>2011</strong> :  +500 citations (:sparkles: +1000)</li>
<li><strong>2010</strong> :  +600 citations (:sparkles: +1200)</li>
</ul>
<p><em>I need your <a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/Contributing.md" target="_blank" rel="external">contributions</a>!</em></p>
<h2 id="Table_of_Contents">Table of Contents</h2><ul>
<li><a href="#survey--review">Survey / Review</a></li>
<li><a href="#theory--future">Theory / Future</a></li>
<li><a href="#optimization--regularization">Optimization / Regularization</a></li>
<li><a href="#network-models">Network Models</a></li>
<li><a href="#image">Image</a></li>
<li><a href="#caption">Caption</a></li>
<li><a href="#video--human-activity">Video / Human Activity</a></li>
<li><a href="#word-embedding">Word Embedding</a></li>
<li><a href="#machine-translation--qna">Machine Translation / QnA</a></li>
<li><a href="#speech--etc">Speech / Etc.</a></li>
<li><a href="#rl--robotics">RL / Robotics</a></li>
<li><a href="#unsupervised">Unsupervised</a></li>
<li><a href="#hardware--software">Hardware / Software</a></li>
</ul>
<p>(Total 82 papers)</p>
<h3 id="Survey_/_Review">Survey / Review</h3><ul>
<li>Deep learning (Book, 2016), Goodfellow et al. <em>(Bengio)</em> <a href="http://www.deeplearningbook.org/" target="_blank" rel="external">[html]</a> </li>
<li>Deep learning (2015), Y. LeCun, Y. Bengio and G. Hinton <a href="http://www.nature.com/nature/journal/v521/n7553/abs/nature14539.html" target="_blank" rel="external">[html]</a> :sparkles:</li>
<li>Deep learning in neural networks: An overview (2015), J. Schmidhuber <a href="http://arxiv.org/pdf/1404.7828" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Representation learning: A review and new perspectives (2013), Y. Bengio et al. <a href="http://arxiv.org/pdf/1206.5538" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
</ul>
<h3 id="Theory_/_Future">Theory / Future</h3><ul>
<li>Distilling the knowledge in a neural network (2015), G. Hinton et al. <a href="http://arxiv.org/pdf/1503.02531" target="_blank" rel="external">[pdf]</a></li>
<li>Deep neural networks are easily fooled: High confidence predictions for unrecognizable images (2015), A. Nguyen et al. <a href="http://arxiv.org/pdf/1412.1897" target="_blank" rel="external">[pdf]</a></li>
<li>How transferable are features in deep neural networks? (2014), J. Yosinski et al. <em>(Bengio)</em> <a href="http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf" target="_blank" rel="external">[pdf]</a></li>
<li>Why does unsupervised pre-training help deep learning (2010), E. Erhan et al. <em>(Bengio)</em> <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_ErhanCBV10.pdf" target="_blank" rel="external">[pdf]</a></li>
<li>Understanding the difficulty of training deep feedforward neural networks (2010), X. Glorot and Y. Bengio <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf" target="_blank" rel="external">[pdf]</a></li>
</ul>
<h3 id="Optimization_/_Regularization">Optimization / Regularization</h3><ul>
<li>Taking the human out of the loop: A review of bayesian optimization (2016), B. Shahriari et al. <a href="https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf" target="_blank" rel="external">[pdf]</a></li>
<li>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (2015), S. Loffe and C. Szegedy <a href="http://arxiv.org/pdf/1502.03167" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Delving deep into rectifiers: Surpassing human-level performance on imagenet classification (2015), K. He et al. <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Dropout: A simple way to prevent neural networks from overfitting (2014), N. Srivastava et al. <em>(Hinton)</em> <a href="http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Adam: A method for stochastic optimization (2014), D. Kingma and J. Ba <a href="http://arxiv.org/pdf/1412.6980" target="_blank" rel="external">[pdf]</a></li>
<li>Spatial pyramid pooling in deep convolutional networks for visual recognition (2014), K. He et al. <a href="http://arxiv.org/pdf/1406.4729" target="_blank" rel="external">[pdf]</a></li>
<li>Regularization of neural networks using dropconnect (2013), L. Wan et al. <em>(LeCun)</em> <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_wan13.pdf" target="_blank" rel="external">[pdf]</a></li>
<li>Improving neural networks by preventing co-adaptation of feature detectors (2012), G. Hinton et al. <a href="http://arxiv.org/pdf/1207.0580.pdf" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Random search for hyper-parameter optimization (2012) J. Bergstra and Y. Bengio <a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a" target="_blank" rel="external">[pdf]</a></li>
</ul>
<h3 id="Network_Models">Network Models</h3><ul>
<li>Deep residual learning for image recognition (2016), K. He et al. <em>(Microsoft)</em> <a href="http://arxiv.org/pdf/1512.03385" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Going deeper with convolutions (2015), C. Szegedy et al. <em>(Google)</em> <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Fast R-CNN (2015), R. Girshick <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Fully convolutional networks for semantic segmentation (2015), J. Long et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Very deep convolutional networks for large-scale image recognition (2014), K. Simonyan and A. Zisserman <a href="http://arxiv.org/pdf/1409.1556" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>OverFeat: Integrated recognition, localization and detection using convolutional networks (2014), P. Sermanet et al. <em>(LeCun)</em> <a href="http://arxiv.org/pdf/1312.6229" target="_blank" rel="external">[pdf]</a></li>
<li>Visualizing and understanding convolutional networks (2014), M. Zeiler and R. Fergus <a href="http://arxiv.org/pdf/1311.2901" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Maxout networks (2013), I. Goodfellow et al. <em>(Bengio)</em> <a href="http://arxiv.org/pdf/1302.4389v4" target="_blank" rel="external">[pdf]</a></li>
<li>ImageNet classification with deep convolutional neural networks (2012), A. Krizhevsky et al. <em>(Hinton)</em> <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Large scale distributed deep networks (2012), J. Dean et al. <a href="http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Deep sparse rectifier neural networks (2011), X. Glorot et al. <em>(Bengio)</em> <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf" target="_blank" rel="external">[pdf]</a></li>
</ul>
<h3 id="Image">Image</h3><ul>
<li>Imagenet large scale visual recognition challenge (2015), O. Russakovsky et al. <a href="http://arxiv.org/pdf/1409.0575" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (2015), S. Ren et al. <a href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf" target="_blank" rel="external">[pdf]</a>  :sparkles:</li>
<li>DRAW: A recurrent neural network for image generation (2015), K. Gregor et al. <a href="http://arxiv.org/pdf/1502.04623" target="_blank" rel="external">[pdf]</a></li>
<li>Rich feature hierarchies for accurate object detection and semantic segmentation (2014), R. Girshick et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Learning and transferring mid-Level image representations using convolutional neural networks (2014), M. Oquab et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf" target="_blank" rel="external">[pdf]</a></li>
<li>DeepFace: Closing the Gap to Human-Level Performance in Face Verification (2014), Y. Taigman et al. <em>(Facebook)</em> <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Decaf: A deep convolutional activation feature for generic visual recognition (2013), J. Donahue et al. <a href="http://arxiv.org/pdf/1310.1531" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Learning Hierarchical Features for Scene Labeling (2013), C. Farabet et al. <em>(LeCun)</em> <a href="https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf" target="_blank" rel="external">[pdf]</a></li>
<li>Learning mid-level features for recognition (2010), Y. Boureau <em>(LeCun)</em> <a href="http://ece.duke.edu/~lcarin/boureau-cvpr-10.pdf" target="_blank" rel="external">[pdf]</a></li>
</ul>
<h3 id="Caption">Caption</h3><ul>
<li>Show, attend and tell: Neural image caption generation with visual attention (2015), K. Xu et al. <em>(Bengio)</em> <a href="http://arxiv.org/pdf/1502.03044" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Show and tell: A neural image caption generator (2015), O. Vinyals et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Long-term recurrent convolutional networks for visual recognition and description (2015), J. Donahue et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Deep visual-semantic alignments for generating image descriptions (2015), A. Karpathy and L. Fei-Fei <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.html" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
</ul>
<h3 id="Video_/_Human_Activity">Video / Human Activity</h3><ul>
<li>Large-scale video classification with convolutional neural networks (2014), A. Karpathy et al. <em>(FeiFei)</em> <a href="vision.stanford.edu/pdf/karpathy14.pdf">[pdf]</a> :sparkles:</li>
<li>DeepPose: Human pose estimation via deep neural networks (2014), A. Toshev and C. Szegedy <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Toshev_DeepPose_Human_Pose_2014_CVPR_paper.pdf" target="_blank" rel="external">[pdf]</a></li>
<li>A survey on human activity recognition using wearable sensors (2013), O. Lara and M. Labrador <a href="http://romisatriawahono.net/lecture/rm/survey/computer%20vision/Lara%20-%20Human%20Activity%20Recognition%20-%202013.pdf" target="_blank" rel="external">[pdf]</a></li>
<li>3D convolutional neural networks for human action recognition (2013), S. Ji et al. <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_JiXYY10.pdf" target="_blank" rel="external">[pdf]</a></li>
<li>Action recognition with improved trajectories (2013), H. Wang and C. Schmid <a href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Wang_Action_Recognition_with_2013_ICCV_paper.pdf" target="_blank" rel="external">[pdf]</a></li>
<li>Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis (2011), Q. Le et al. <a href="http://robotics.stanford.edu/~wzou/cvpr_LeZouYeungNg11.pdf" target="_blank" rel="external">[pdf]</a></li>
</ul>
<h3 id="Word_Embedding">Word Embedding</h3><ul>
<li>Glove: Global vectors for word representation (2014), J. Pennington et al. <a href="http://llcao.net/cu-deeplearning15/presentation/nn-pres.pdf" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Distributed representations of sentences and documents (2014), Q. Le and T. Mikolov <a href="http://arxiv.org/pdf/1405.4053" target="_blank" rel="external">[pdf]</a> <em>(Google)</em> :sparkles:</li>
<li>Distributed representations of words and phrases and their compositionality (2013), T. Mikolov et al. <em>(Google)</em> <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Efficient estimation of word representations in vector space (2013), T. Mikolov et al. <em>(Google)</em> <a href="http://arxiv.org/pdf/1301.3781" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Word representations: a simple and general method for semi-supervised learning (2010), J. Turian <em>(Bengio)</em> <a href="http://www.anthology.aclweb.org/P/P10/P10-1040.pdf" target="_blank" rel="external">[pdf]</a></li>
</ul>
<h3 id="Machine_Translation_/_QnA">Machine Translation / QnA</h3><ul>
<li>Towards ai-complete question answering: A set of prerequisite toy tasks (2015), J. Weston et al. <a href="http://arxiv.org/pdf/1502.05698" target="_blank" rel="external">[pdf]</a></li>
<li>Neural machine translation by jointly learning to align and translate (2014), D. Bahdanau et al. <em>(Bengio)</em> <a href="http://arxiv.org/pdf/1409.0473" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Sequence to sequence learning with neural networks (2014), I. Sutskever et al. <a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="external">[pdf]</a></li>
<li>Learning phrase representations using RNN encoder-decoder for statistical machine translation (2014), K. Cho et al. <em>(Bengio)</em> <a href="http://arxiv.org/pdf/1406.1078" target="_blank" rel="external">[pdf]</a></li>
<li>A convolutional neural network for modelling sentences (2014), N. kalchbrenner et al. <a href="http://arxiv.org/pdf/1404.2188v1" target="_blank" rel="external">[pdf]</a></li>
<li>Convolutional neural networks for sentence classification (2014), Y. Kim <a href="http://arxiv.org/pdf/1408.5882" target="_blank" rel="external">[pdf]</a></li>
<li>The stanford coreNLP natural language processing toolkit (2014), C. Manning et al. <a href="http://www.surdeanu.info/mihai/papers/acl2014-corenlp.pdf" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Recursive deep models for semantic compositionality over a sentiment treebank (2013), R. Socher et al. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.383.1327&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Natural language processing (almost) from scratch (2011), R. Collobert et al. <a href="http://arxiv.org/pdf/1103.0398" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Recurrent neural network based language model (2010), T. Mikolov et al. <a href="http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf" target="_blank" rel="external">[pdf]</a></li>
</ul>
<h3 id="Speech_/_Etc-">Speech / Etc.</h3><ul>
<li>Speech recognition with deep recurrent neural networks (2013), A. Graves <em>(Hinton)</em> <a href="http://arxiv.org/pdf/1303.5778.pdf" target="_blank" rel="external">[pdf]</a></li>
<li>Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups (2012), G. Hinton et al. <a href="http://www.cs.toronto.edu/~asamir/papers/SPM_DNN_12.pdf" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition (2012) G. Dahl et al. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.337.7548&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Acoustic modeling using deep belief networks (2012), A. Mohamed et al. <em>(Hinton)</em> <a href="http://www.cs.toronto.edu/~asamir/papers/speechDBN_jrnl.pdf" target="_blank" rel="external">[pdf]</a></li>
</ul>
<h3 id="RL_/_Robotics">RL / Robotics</h3><ul>
<li>Mastering the game of Go with deep neural networks and tree search, D. Silver et al. <em>(DeepMind)</em> <a href="Mastering the game of Go with deep neural networks and tree search">[pdf]</a></li>
<li>Human-level control through deep reinforcement learning (2015), V. Mnih et al. <em>(DeepMind)</em> <a href="http://www.davidqiu.com:8888/research/nature14236.pdf" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Deep learning for detecting robotic grasps (2015), I. Lenz et al. <a href="http://www.cs.cornell.edu/~asaxena/papers/lenz_lee_saxena_deep_learning_grasping_ijrr2014.pdf" target="_blank" rel="external">[pdf]</a></li>
<li>Playing atari with deep reinforcement learning (2013), V. Mnih et al. <em>(DeepMind)</em> <a href="http://arxiv.org/pdf/1312.5602.pdf" target="_blank" rel="external">[pdf]</a>)</li>
</ul>
<h3 id="Unsupervised">Unsupervised</h3><ul>
<li>Generative adversarial nets (2014), I. Goodfellow et al. <em>(Bengio)</em> <a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" target="_blank" rel="external">[pdf]</a></li>
<li>Auto-Encoding Variational Bayes (2013), D. Kingma and M. Welling <a href="http://arxiv.org/pdf/1312.6114" target="_blank" rel="external">[pdf]</a></li>
<li>Building high-level features using large scale unsupervised learning (2013), Q. Le et al. <a href="http://arxiv.org/pdf/1112.6209" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Contractive auto-encoders: Explicit invariance during feature extraction (2011), S. Rifai et al. <em>(Bengio)</em> <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Rifai_455.pdf" target="_blank" rel="external">[pdf]</a></li>
<li>An analysis of single-layer networks in unsupervised feature learning (2011), A. Coates et al. <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_CoatesNL11.pdf" target="_blank" rel="external">[pdf]</a></li>
<li>Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion (2010), P. Vincent et al. <em>(Bengio)</em> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.3484&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">[pdf]</a></li>
<li>A practical guide to training restricted boltzmann machines (2010), G. Hinton <a href="http://www.csri.utoronto.ca/~hinton/absps/guideTR.pdf" target="_blank" rel="external">[pdf]</a></li>
<li>Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion (2010), P. Vincent et al. <em>(Bengio)</em> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.3484&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">[pdf]</a></li>
</ul>
<h3 id="Hardware_/_Software">Hardware / Software</h3><ul>
<li>TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems (2016), M. Abadi et al. <em>(Google)</em> <a href="http://arxiv.org/pdf/1603.04467" target="_blank" rel="external">[pdf]</a></li>
<li>MatConvNet: Convolutional neural networks for matlab (2015), A. Vedaldi and K. Lenc <a href="http://arxiv.org/pdf/1412.4564" target="_blank" rel="external">[pdf]</a></li>
<li>Caffe: Convolutional architecture for fast feature embedding (2014), Y. Jia et al. <a href="http://arxiv.org/pdf/1408.5093" target="_blank" rel="external">[pdf]</a> :sparkles:</li>
<li>Theano: new features and speed improvements (2012), F. Bastien et al. <em>(Bengio)</em> <a href="http://arxiv.org/pdf/1211.5590" target="_blank" rel="external">[pdf]</a></li>
</ul>
<h2 id="License">License</h2><p><a href="https://creativecommons.org/publicdomain/zero/1.0/" target="_blank" rel="external"><img src="http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg" alt="CC0"></a></p>
<p>To the extent possible under law, <a href="https://www.facebook.com/terryum.io/" target="_blank" rel="external">Terry T. Um</a> has waived all copyright and related or neighboring rights to this work.</p>

      
    </div>

    <div class="post-footer">
      

      
        <div class="post-nav">
          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/06/04/ReadingPaperList/">Reading Papers List</a>
            
          </div>

          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/06/04/Contributing/"></a>
            
          </div>
        </div>
      

      
      
    </div>
  </div>



    
      <div class="comments" id="comments">
        
          <div class="ds-thread" data-thread-key="2016/06/04/paper/"
               data-title="" data-url="http://gaobb.github.io/2016/06/04/paper/">
          </div>
        
      </div>
    
  </div>


        </div>

        
      </div>


      
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <div id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview">
        <div class="site-author motion-element">
          <img class="site-author-image" src="http://lamda.nju.edu.cn/gaobb/index_files/gaobb.jpg" alt="Bin-Bin Gao" />
          <p class="site-author-name">Bin-Bin Gao</p>
        </div>
        <p class="site-description motion-element"></p>
        <div class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">20</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">8</span>
              <span class="site-state-item-name">分类</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">27</span>
              <span class="site-state-item-name">标签</span>
              </a>
          </div>

        </div>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
              <a href="https://github.com/gaobb" target="_blank">GitHub</a>
            </span>
            
              <span class="links-of-author-item">
              <a href="http://weibo.com/csgaobb" target="_blank">微博</a>
            </span>
            
              <span class="links-of-author-item">
              <a href="https://www.zhihu.com/people/csgaobb" target="_blank">知乎</a>
            </span>
            
              <span class="links-of-author-item">
              <a href="http://lamda.nju.edu.cn/gaobb/" target="_blank">HomePage</a>
            </span>
            
          
        </div>

        
        

      </div>

      
        <div class="post-toc-wrap sidebar-panel-active">
          <div class="post-toc-indicator-top post-toc-indicator"></div>
          <div class="post-toc">
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Awesome_-_Most_Cited_Deep_Learning_Papers"><span class="nav-number">1.</span> <span class="nav-text">Awesome - Most Cited Deep Learning Papers</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Awesome_list_criteria"><span class="nav-number">1.1.</span> <span class="nav-text">Awesome list criteria</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Table_of_Contents"><span class="nav-number">1.2.</span> <span class="nav-text">Table of Contents</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Survey_/_Review"><span class="nav-number">1.2.1.</span> <span class="nav-text">Survey / Review</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Theory_/_Future"><span class="nav-number">1.2.2.</span> <span class="nav-text">Theory / Future</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Optimization_/_Regularization"><span class="nav-number">1.2.3.</span> <span class="nav-text">Optimization / Regularization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Network_Models"><span class="nav-number">1.2.4.</span> <span class="nav-text">Network Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Image"><span class="nav-number">1.2.5.</span> <span class="nav-text">Image</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Caption"><span class="nav-number">1.2.6.</span> <span class="nav-text">Caption</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Video_/_Human_Activity"><span class="nav-number">1.2.7.</span> <span class="nav-text">Video / Human Activity</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Word_Embedding"><span class="nav-number">1.2.8.</span> <span class="nav-text">Word Embedding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Machine_Translation_/_QnA"><span class="nav-number">1.2.9.</span> <span class="nav-text">Machine Translation / QnA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Speech_/_Etc-"><span class="nav-number">1.2.10.</span> <span class="nav-text">Speech / Etc.</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RL_/_Robotics"><span class="nav-number">1.2.11.</span> <span class="nav-text">RL / Robotics</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Unsupervised"><span class="nav-number">1.2.12.</span> <span class="nav-text">Unsupervised</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hardware_/_Software"><span class="nav-number">1.2.13.</span> <span class="nav-text">Hardware / Software</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#License"><span class="nav-number">1.3.</span> <span class="nav-text">License</span></a></li></ol></li></ol></div>
            
          </div>
          <div class="post-toc-indicator-bottom post-toc-indicator"></div>
        </div>
      

    </div>
  </div>


    </div>

    <div id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; &nbsp; 
  2016
  <span class="with-love">
    <i class="icon-heart"></i>
  </span>
  <span class="author">Bin-Bin Gao</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>



      </div>
    </div>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js"></script>


  <script type="text/javascript" src="/js/helpers.js"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js" id="motion.global"></script>




  
  
<script type="text/javascript" src="/js/bootstrap.scrollspy.js" id="bootstrap.scrollspy.custom"></script>


<script type="text/javascript" id="sidebar.toc.highlight">
  $(document).ready(function () {
    var tocSelector = '.post-toc';
    var $tocSelector = $(tocSelector);
    var activeCurrentSelector = '.active-current';

    $tocSelector
      .on('activate.bs.scrollspy', function () {
        var $currentActiveElement = $(tocSelector + ' .active').last();

        removeCurrentActiveClass();
        $currentActiveElement.addClass('active-current');

        $tocSelector[0].scrollTop = $currentActiveElement.position().top;
      })
      .on('clear.bs.scrollspy', function () {
        removeCurrentActiveClass();
      });

    function removeCurrentActiveClass () {
      $(tocSelector + ' ' + activeCurrentSelector)
        .removeClass(activeCurrentSelector.substring(1));
    }

    function processTOC () {
      getTOCMaxHeight();
      toggleTOCOverflowIndicators();
    }

    function getTOCMaxHeight () {
      var $sidebarInner = $('.sidebar-inner');
      var height = $('.sidebar').height() -
                   $tocSelector.position().top -
                   $('.post-toc-indicator-bottom').height();

      $tocSelector.css('height', height);

      return height;
    }

    function toggleTOCOverflowIndicators () {
      tocOverflowIndicator(
        '.post-toc-indicator-top',
        $tocSelector.scrollTop() > 0 ? 'show' : 'hide'
      );

      tocOverflowIndicator(
        '.post-toc-indicator-bottom',
        $tocSelector.scrollTop() >= $tocSelector.find('ol').height() - $tocSelector.height() ? 'hide' : 'show'
      )
    }

    $(document).on('sidebar.didShow', function () {
      processTOC();
    });

    $('body').scrollspy({ target: tocSelector });
    $(window).on('resize', function () {
      if ( $('.sidebar').hasClass('sidebar-active') ) {
        processTOC();
      }
    });

    onScroll($tocSelector);

    function onScroll (element) {
      element.on('mousewheel DOMMouseScroll', function (event) {
          var oe = event.originalEvent;
          var delta = oe.wheelDelta || -oe.detail;
          var self = this;

          this.scrollTop += ( delta < 0 ? 1 : -1 ) * 30;
          event.preventDefault();

          toggleTOCOverflowIndicators();
      });
    }

    function tocOverflowIndicator (indicator, action) {
      $(indicator).velocity('stop').velocity({
        opacity: action === 'show' ? 0.4 : 0
      }, { duration: 100 });
    }

  });
</script>


  <script type="text/javascript" id="sidebar.nav">
    $(document).ready(function () {
      var html = $('html');

      $('.sidebar-nav li').on('click', function () {
        var item = $(this);
        var activeTabClassName = 'sidebar-nav-active';
        var activePanelClassName = 'sidebar-panel-active';
        if (item.hasClass(activeTabClassName)) {
          return;
        }

        var currentTarget = $('.' + activePanelClassName);
        var target = $('.' + item.data('target'));

        currentTarget.velocity('transition.slideUpOut', 200, function () {
          target
            .velocity('stop')
            .velocity('transition.slideDownIn', 200)
            .addClass(activePanelClassName);
        });

        item.siblings().removeClass(activeTabClassName);
        item.addClass(activeTabClassName);
      });

      $('.post-toc a').on('click', function (e) {
        e.preventDefault();
        var offset = $(escapeSelector(this.getAttribute('href'))).offset().top;
        html.velocity('stop').velocity('scroll', {
          offset: offset  + 'px',
          mobileHA: false
        });
      });

      // Expand sidebar on post detail page by default, when post has a toc.
      var $tocContent = $('.post-toc-content');
      if (isDesktop() && CONFIG.sidebar === 'post') {
        if ($tocContent.length > 0 && $tocContent.html().trim().length > 0) {
          displaySidebar();
        }
      }
    });
  </script>




  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
    });
  </script>

  

  
  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"gaobb"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  


  
  

</body>
</html>
